**A transformer has two parts -- ``ENCODER & DECODER``**

![image](https://github.com/user-attachments/assets/0529067c-6ac6-4758-8907-7c148efb0e4e)

* In the original paper, they used 6 Encoder and 6 Decoder blocks.

![image](https://github.com/user-attachments/assets/2c668ad3-6d7d-4bb7-90bb-0f7dfa6a1f54)

* Each ENCODER block has -- FEED FORWARD NEURAL NETWORK & SELF ATTENTION
 
![image](https://github.com/user-attachments/assets/a8901788-9bb1-4d50-bb33-fe419e8e77bf)
---------------------------------


Consider we have an INPUT BOX -- then 6 ENCODER BLOCKS  -- and then OUTPUT BLOCK:

![image](https://github.com/user-attachments/assets/5ca9da85-3186-49e0-a03e-2ae119cd3534)

* In the INPUT BOX, 3 steps take place:

![image](https://github.com/user-attachments/assets/5f9e1ccf-d473-4c42-b1ed-a9c39151f10d)
![image](https://github.com/user-attachments/assets/c43a8317-4138-45d3-8988-c7c0e0c2eaf4)

----------------
**Next, we are going to focus on this thing first:**

![image](https://github.com/user-attachments/assets/2f334b86-6017-4792-958b-3c81b2521e4a)

``Below is the expanded view of it:``

![image](https://github.com/user-attachments/assets/3a8057ea-3b94-4109-b37c-968200aeb043)
![image](https://github.com/user-attachments/assets/7af67ed1-8562-4d76-b993-650760eeef93)
![image](https://github.com/user-attachments/assets/43717298-d64f-4d18-a308-b384f8f67bc6)
![image](https://github.com/user-attachments/assets/a7a58ba4-fd54-4694-9bc9-3c17ea20531a)





